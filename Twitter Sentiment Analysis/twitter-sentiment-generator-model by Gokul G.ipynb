{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2510329,"sourceType":"datasetVersion","datasetId":1520310}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":29.465159,"end_time":"2024-04-30T13:49:57.674514","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-30T13:49:28.209355","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://th.bing.com/th/id/OIG2.sONQejaDIjDVlciFyay8?pid=ImgGn\" style=\"width:100%\">","metadata":{}},{"cell_type":"markdown","source":"<div style=\"font-family: Arial, sans-serif; padding: 20px; background-color: #F0FFFF; color: navy; border: 2px solid navy; border-radius: 10px; width: 100%;\">\n  <div style=\"text-align: center;\">\n    <h1 style=\"font-size: 28px;\">üì£ Basic Information</h1>\n  </div>\n  <div style=\"padding: 20px;\">\n    <div style=\"margin-bottom: 20px;\">\n      <h2 style=\"font-size: 24px;\">üîª About Author</h2>\n      <p style=\"font-size: 18px;\">\n        Hi üëã everyone! Welcome to my notebook. üìì My name is Gokul Gopalan Achary and I'm passionate about data science, and I'm excited to share my findings with you. ü§ì In my recent notebook, I successfully implemented a RandomForestClassifier on the Twitter Sentiment dataset. If you find this notebook helpful, please Upvote! ‚ù§Ô∏è<br><br>\n        ü§ù I am eager to cultivate enriching collaborations, foster knowledge exchange, and forge strategic alliances within the vibrant data science community. Join me on Kaggle or any other suitable platform to co-create, ideate, and elevate our collective expertise. Let's embark on a journey of innovation and discovery together!\n      </p>\n    </div>\n    <div>\n      <h2 style=\"font-size: 24px;\">üîª About Model</h2>\n      <p style=\"font-size: 18px;\">\n        üå≥ <b>RandomForestClassifier</b> <br>\n        RandomForestClassifier is a supervised machine learning algorithm that is part of the ensemble learning method. It is used for classification problems.<br>\n        Here's how the RandomForestClassifier works: <br><br>\n        <b>Bagging</b>: üéí The algorithm starts by creating multiple decision tree models on random subsets of the training data using a technique called bagging (bootstrap aggregating). This creates a \"forest\" of decision trees. <br><br>\n        <b>Random Feature Selection</b>: üé≤ When splitting a node during the construction of a tree, the algorithm selects a random subset of features to consider, rather than considering all features, as is done in a standard decision tree. This adds an additional layer of randomness to the model. <br><br>\n        <b>Voting/Averaging</b>: üó≥Ô∏è Once the forest of decision trees is created, to classify a new data point, the algorithm will pass the data point through each of the decision trees in the forest. Each tree will provide a classification output. The final classification is determined by a majority vote (for classification problems) or an average (for regression problems) of all the individual tree predictions. <br><br>\n        The key advantages of the RandomForestClassifier are: <br>\n        <ul style=\"list-style-type: disc; margin-left: 20px;\">\n          <li>It can handle both classification and regression problems.</li>\n          <li>It is robust to outliers and can handle missing values well.</li>\n          <li>It has a high accuracy compared to a single decision tree.</li>\n          <li>It can automatically handle feature importance, making it useful for feature selection.</li>\n          <li>It is relatively simple to tune and interpret compared to other complex models like neural networks.</li>\n        </ul>\n      </p>\n    </div>\n    <div>\n      <h2 style=\"font-size: 24px;\">üîª Table of Contents</h2>\n      <ul style=\"list-style-type: none;\">\n        <li><a href=\"#1\">Step 1 | Libraries</a>\n          <ul style=\"list-style-type: none;\">\n            <li><a href=\"#1_1\">Load Data</a></li>\n            <li><a href=\"#1_2\">Data Info</a></li>\n            <li><a href=\"#1_3\">Rename Columns</a></li>\n            <li><a href=\"#1_4\">Deal Missing Values</a></li>\n            <li><a href=\"#1_5\">Deal Duplicate Values</a></li>\n          </ul>\n        </li>\n        <li><a href=\"#2\">Step 2 | Visualization</a>\n          <ul style=\"list-style-type: none;\">\n            <li><a href=\"#2_1\">Pie & Bar</a></li>\n            <li><a href=\"#2_2\">Cross Table Plot</a></li>\n            <li><a href=\"#2_3\">Cloud Plot Of Branches</a></li>\n            <li><a href=\"#2_4\">Cloud Plot Of Tweets</a></li>\n          </ul>\n        </li>\n        <li><a href=\"#3\">Step 3 | Preprocessing</a>\n          <ul style=\"list-style-type: none;\">\n            <li><a href=\"#3_1\">Remove Non String</a></li>\n            <li><a href=\"#3_2\">Convert In LowerCase</a></li>\n            <li><a href=\"#3_3\">Remove Html Tags</a></li>\n            <li><a href=\"#3_4\">Remove URL</a></li>\n            <li><a href=\"#3_5\">Remove Numeric Digits</a></li>\n            <li><a href=\"#3_6\">Remove Punctuation</a></li>\n            <li><a href=\"#3_7\">Split Text In Token</a></li>\n            <li><a href=\"#3_8\">Eliminate Stop Words</a></li>\n            <li><a href=\"#3_9\">Remove Emoji</a></li>\n            <li><a href=\"#3_10\">Vectorized Data</a></li>\n          </ul>\n        </li>\n        <li><a href=\"#4\">Step 4 | Build Model</a>\n          <ul style=\"list-style-type: none;\">\n            <li><a href=\"#4_1\">Train Split Data</a></li>\n            <li><a href=\"#4_2\">RandomForestClassifier</a></li>\n            <li><a href=\"#4_3\">Confusion Matrix</a></li>\n            <li><a href=\"#4_4\">Actual VS Predicted</a></li>\n            <li><a href=\"#4_5\">Save Model</a></li>\n            <li><a href=\"#4_6\">Predict Random Tweet By Save Model</a></li>\n          </ul>\n        </li>\n      </ul>\n    </div>\n  </div>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <b> <span style='color:dimgray'>Step 1</span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Libraries</span></b>\n","metadata":{}},{"cell_type":"code","source":"# import libraries\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport re\nimport string\nimport nltk\nimport joblib\n# Download the WordNet resource\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report","metadata":{"papermill":{"duration":2.001964,"end_time":"2024-04-30T13:49:32.837959","exception":false,"start_time":"2024-04-30T13:49:30.835995","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-05-03T08:04:54.63575Z","iopub.execute_input":"2024-05-03T08:04:54.636121Z","iopub.status.idle":"2024-05-03T08:04:57.446688Z","shell.execute_reply.started":"2024-05-03T08:04:54.636091Z","shell.execute_reply":"2024-05-03T08:04:57.445497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1_1\"></a>\n# <b> <span style='color:dimgray'>Step 1.1</span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Load Data</span></b>\n","metadata":{}},{"cell_type":"code","source":"# read data\ndf_train = pd.read_csv('/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv')\ndf_val = pd.read_csv('/kaggle/input/twitter-entity-sentiment-analysis/twitter_validation.csv')\n# concatenate data\ndf = pd.concat([df_train, df_val], ignore_index=False)\ndf.head()","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:28.441148Z","iopub.status.busy":"2024-05-03T05:22:28.440759Z","iopub.status.idle":"2024-05-03T05:22:28.767881Z","shell.execute_reply":"2024-05-03T05:22:28.766761Z","shell.execute_reply.started":"2024-05-03T05:22:28.441114Z"},"papermill":{"duration":0.438673,"end_time":"2024-04-30T13:49:33.28309","exception":false,"start_time":"2024-04-30T13:49:32.844417","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1_2\"></a>\n# <b> <span style='color:dimgray'>Step 1.2</span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Data Info</span></b>","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:28.769751Z","iopub.status.busy":"2024-05-03T05:22:28.769381Z","iopub.status.idle":"2024-05-03T05:22:28.823247Z","shell.execute_reply":"2024-05-03T05:22:28.822003Z","shell.execute_reply.started":"2024-05-03T05:22:28.769721Z"},"papermill":{"duration":0.051438,"end_time":"2024-04-30T13:49:33.375591","exception":false,"start_time":"2024-04-30T13:49:33.324153","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1_3\"></a>\n# <b> <span style='color:dimgray'>Step 1.3</span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Rename Columns</span></b>\n","metadata":{}},{"cell_type":"code","source":"# Remove unnecessary columns\ncolumns_to_drop = ['2401', '3364', 'Facebook', 'Irrelevant',\n                   'I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom‚Äôs great auntie as ‚ÄòHayley can‚Äôt get out of bed‚Äô and told to his grandma, who now thinks I‚Äôm a lazy, terrible person ü§£']\ndf = df.drop(columns_to_drop, axis=1)\n\n# Rename columns\ndf = df.rename(columns={'im getting on borderlands and i will murder you all ,': 'Tweet', 'Positive': 'Sentiment', 'Borderlands':\"Branch\"})\ndf.head()","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:28.827768Z","iopub.status.busy":"2024-05-03T05:22:28.826677Z","iopub.status.idle":"2024-05-03T05:22:28.853199Z","shell.execute_reply":"2024-05-03T05:22:28.851885Z","shell.execute_reply.started":"2024-05-03T05:22:28.82772Z"},"papermill":{"duration":0.023657,"end_time":"2024-04-30T13:49:33.42908","exception":false,"start_time":"2024-04-30T13:49:33.405423","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1_4\"></a>\n# <b> <span style='color:dimgray'>Step 1.4</span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Check Missing Values</span></b>\n","metadata":{}},{"cell_type":"code","source":"# check missing values\nprint(df.isnull().sum())","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:28.855207Z","iopub.status.busy":"2024-05-03T05:22:28.85478Z","iopub.status.idle":"2024-05-03T05:22:28.889438Z","shell.execute_reply":"2024-05-03T05:22:28.888237Z","shell.execute_reply.started":"2024-05-03T05:22:28.855175Z"},"papermill":{"duration":0.025681,"end_time":"2024-04-30T13:49:33.488061","exception":false,"start_time":"2024-04-30T13:49:33.46238","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove missing values\ndf.dropna(inplace=True)\n# check missing values\ndf.isnull().sum()   ","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:28.891243Z","iopub.status.busy":"2024-05-03T05:22:28.890885Z","iopub.status.idle":"2024-05-03T05:22:28.963321Z","shell.execute_reply":"2024-05-03T05:22:28.96216Z","shell.execute_reply.started":"2024-05-03T05:22:28.891203Z"},"papermill":{"duration":0.041445,"end_time":"2024-04-30T13:49:33.536355","exception":false,"start_time":"2024-04-30T13:49:33.49491","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1_5\"></a>\n# <b> <span style='color:dimgray'>Step 1.5</span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Check Duplicate Values</span></b>\n","metadata":{}},{"cell_type":"code","source":"# check duplicate values\ndf.duplicated().sum()","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:28.965179Z","iopub.status.busy":"2024-05-03T05:22:28.964838Z","iopub.status.idle":"2024-05-03T05:22:29.034235Z","shell.execute_reply":"2024-05-03T05:22:29.033007Z","shell.execute_reply.started":"2024-05-03T05:22:28.965149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove duplicate values \nremove_duplicates = df.drop_duplicates()\ndf = remove_duplicates\n# check duplicate values\ndf.duplicated().sum()","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:29.035992Z","iopub.status.busy":"2024-05-03T05:22:29.035657Z","iopub.status.idle":"2024-05-03T05:22:29.159847Z","shell.execute_reply":"2024-05-03T05:22:29.158764Z","shell.execute_reply.started":"2024-05-03T05:22:29.035954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <b> <span style='color:dimgray'>Step 2</span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Visualization</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2_1\"></a>\n# <b> <span style='color:dimgray'>Step 2.1 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Pie & Bar Plot</span></b>\n","metadata":{}},{"cell_type":"code","source":"# Define custom colors\ncolors = ['red', 'green', 'blue', 'gray']\n\n# Create subplots\nfig, axs = plt.subplots(1, 2, figsize=(16, 6))\n\n# Plotting pie chart for sentiment distribution with custom colors\naxs[0].pie(df['Sentiment'].value_counts(), labels=df['Sentiment'].unique(), autopct='%1.1f%%',\n            startangle=90, wedgeprops={'linewidth': 0.5}, textprops={'fontsize': 12},\n            explode=[0.1, 0.1, 0.1, 0.1], colors=colors, shadow=True)\naxs[0].set_title('Sentiment Distribution - Pie Chart')\n\n# Plotting bar plot for sentiment distribution\naxs[1] = df['Sentiment'].value_counts().plot(kind='bar', color=colors, ax=axs[1])\naxs[1].set_title('Sentiment Distribution - Bar Plot')\naxs[1].set_xlabel('Sentiment')\naxs[1].set_ylabel('Count')\naxs[1].tick_params(axis='x', rotation=45)\naxs[1].grid(axis='y', linestyle='--', alpha=0.7)\n\n# Add text on top of each bar in the bar plot\nfor p in axs[1].patches:\n    axs[1].annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:29.165131Z","iopub.status.busy":"2024-05-03T05:22:29.164779Z","iopub.status.idle":"2024-05-03T05:22:29.767821Z","shell.execute_reply":"2024-05-03T05:22:29.76653Z","shell.execute_reply.started":"2024-05-03T05:22:29.165101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2_2\"></a>\n# <b> <span style='color:dimgray'>Step 2.2</span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Cross Table</span></b>\n","metadata":{}},{"cell_type":"code","source":"# Create cross-tabulation table in plot\nplt.figure(figsize=(10, 6))\ncount_table = pd.crosstab(index=df['Branch'], columns=df['Sentiment'])\nsns.heatmap(count_table, cmap='YlOrRd', annot=True, fmt='d',linewidths=0.5, linecolor='black')\nplt.title('Sentiment Distribution by Branch')\nplt.xlabel('Sentiment')\nplt.ylabel('Branch')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:29.769571Z","iopub.status.busy":"2024-05-03T05:22:29.769202Z","iopub.status.idle":"2024-05-03T05:22:30.7594Z","shell.execute_reply":"2024-05-03T05:22:30.758151Z","shell.execute_reply.started":"2024-05-03T05:22:29.769539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display minimum and maximum values for each sentiment for each branch\nfor sentiment in count_table.columns:\n    min_branch = count_table[sentiment].idxmin()\n    max_branch = count_table[sentiment].idxmax()\n    min_val = count_table[sentiment].min()\n    max_val = count_table[sentiment].max()\n    print(f\"Sentiment '{sentiment}':\")\n    print(f\"  Minimum value '{min_val}' at Branch '{min_branch}'\")\n    print(f\"  Maximum value '{max_val}' at Branch '{max_branch}'\\n\")\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:30.761412Z","iopub.status.busy":"2024-05-03T05:22:30.760951Z","iopub.status.idle":"2024-05-03T05:22:30.772167Z","shell.execute_reply":"2024-05-03T05:22:30.771021Z","shell.execute_reply.started":"2024-05-03T05:22:30.761374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Sentiment : **Positive**\\\n  Minimum value **154** at Branch **'Facebook'**\\\n  Maximum value **1372** at Branch **'AssassinsCreed'**\n  \n \n> Sentiment : **Negative**\\\n  Minimum value **289** at Branch **'RedDeadRedemption(RDR)'**\\\n  Maximum value **1647** at Branch **'MaddenNFL'** \n  \n> Sentiment : **Neutral**\\\n  Minimum value **100** at Branch **'FIFA'**\\\n  Maximum value **1165** at Branch **'Amazon'**\n  \n> Sentiment : **Irrelevant**\\\n  Minimum value **2112** at Branch **'PlayerUnknownsBattlegrounds(PUBG)'**\\\n  Maximum value **2304** at Branch **'CallOfDuty'**\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2_3\"></a>\n# <b> <span style='color:dimgray'>Step 2.3</span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Cloud Plot Of Branch</span></b>\n","metadata":{}},{"cell_type":"code","source":"# Convert branches to a single string\nbranches_text = ' '.join(count_table.index)\n\n# Create word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(branches_text)\n\n# Plot word cloud\nplt.figure(figsize=(10, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Word Cloud of Branches')\nplt.axis('off')\nplt.show()\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:30.77461Z","iopub.status.busy":"2024-05-03T05:22:30.773916Z","iopub.status.idle":"2024-05-03T05:22:31.415202Z","shell.execute_reply":"2024-05-03T05:22:31.413945Z","shell.execute_reply.started":"2024-05-03T05:22:30.774568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2_4\"></a>\n# <b> <span style='color:dimgray'>Step 2.4</span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Cloud Plot Of Tweet</span></b>\n","metadata":{}},{"cell_type":"code","source":"# Concatenate all tweets into a single string\nall_tweets_text = ' '.join(df['Tweet'])\n\n# Create word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_tweets_text)\n\n# Plot word cloud\nplt.figure(figsize=(10, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Word Cloud of Tweets')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:31.417128Z","iopub.status.busy":"2024-05-03T05:22:31.416734Z","iopub.status.idle":"2024-05-03T05:22:39.383256Z","shell.execute_reply":"2024-05-03T05:22:39.382104Z","shell.execute_reply.started":"2024-05-03T05:22:31.417093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <b> <span style='color:dimgray'>Step 3</span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Preprocessing The Data</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3_1\"></a>\n# <b> <span style='color:dimgray'>Step 3.1 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Remove Non-String</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"**Remove Non-String: In this step, any non-string characters or elements are removed from the text. For example, if your text contains numbers or special characters that are not relevant to the classification task, they can be removed.**\n\n**Example:**\\\n**Input: \"The product is priced at $99.\"**\\\n**Output: \"The product is priced at .\"**","metadata":{}},{"cell_type":"code","source":"def filter_non_string(df, column):\n    \"\"\"\n    Filter out rows with non-string values in the specified column.\n    Convert non-string values to strings.\n    \"\"\"\n    df = df.dropna(subset=[column])\n    df[column] = df[column].astype(str)\n    return df\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:39.385027Z","iopub.status.busy":"2024-05-03T05:22:39.384674Z","iopub.status.idle":"2024-05-03T05:22:39.391038Z","shell.execute_reply":"2024-05-03T05:22:39.389733Z","shell.execute_reply.started":"2024-05-03T05:22:39.384995Z"},"papermill":{"duration":0.01671,"end_time":"2024-04-30T13:49:34.717903","exception":false,"start_time":"2024-04-30T13:49:34.701193","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3_2\"></a>\n# <b> <span style='color:dimgray'>Step 3.2 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Convert In LowerCase</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"**Convert to Lowercase:** This step involves converting all the text to lowercase. It helps in treating words with different cases as the same and reduces the vocabulary size.\n\n**Example:**  \n**Input:** \"I LOVE OpenAI.\"  \n**Output:** \"i love openai.\"\n","metadata":{}},{"cell_type":"code","source":"def normalize_text(text):\n    \"\"\"Convert text to lowercase to ensure consistency across the corpus.\"\"\"\n    return text.lower()\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:39.392945Z","iopub.status.busy":"2024-05-03T05:22:39.39257Z","iopub.status.idle":"2024-05-03T05:22:39.403907Z","shell.execute_reply":"2024-05-03T05:22:39.402704Z","shell.execute_reply.started":"2024-05-03T05:22:39.3929Z"},"papermill":{"duration":0.016743,"end_time":"2024-04-30T13:49:34.743528","exception":false,"start_time":"2024-04-30T13:49:34.726785","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3_3\"></a>\n# <b> <span style='color:dimgray'>Step 3.3 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Remove HTML Tags</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"**Remove HTML Tags:** If your text contains HTML tags, such as <p>, <a href=\"...\">, or <b>, you may want to remove them as they usually don't contribute to the classification task.\n\n**Example:**  \n**Input:** \"<p>This is an >><b>important>></b> <<>>message.</p>\"  \n**Output:** \"This is an important message.\"\n","metadata":{}},{"cell_type":"code","source":"def remove_html_tags(text):\n    \"\"\"Remove HTML tags from the text.\"\"\"\n    return re.sub(r'<.*?>', '', text)\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:39.405518Z","iopub.status.busy":"2024-05-03T05:22:39.405149Z","iopub.status.idle":"2024-05-03T05:22:39.416041Z","shell.execute_reply":"2024-05-03T05:22:39.414852Z","shell.execute_reply.started":"2024-05-03T05:22:39.405465Z"},"papermill":{"duration":0.015925,"end_time":"2024-04-30T13:49:34.767428","exception":false,"start_time":"2024-04-30T13:49:34.751503","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3_4\"></a>\n# <b> <span style='color:dimgray'>Step 3.4 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Remove URL Or HyperLink</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"**Remove URLs:** URLs or website links might not provide any useful information for classification and can be removed.\n\n**Example:**  \n**Input:** \"Check out this amazing website: www.example.com\"  \n**Output:** \"Check out this amazing website:\"\n","metadata":{}},{"cell_type":"code","source":"def remove_urls(text):\n    \"\"\"Remove URLs or hyperlinks from the text.\"\"\"\n    return re.sub(r'http\\S+|www\\S+', '', text)\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:39.418093Z","iopub.status.busy":"2024-05-03T05:22:39.417655Z","iopub.status.idle":"2024-05-03T05:22:39.427893Z","shell.execute_reply":"2024-05-03T05:22:39.426878Z","shell.execute_reply.started":"2024-05-03T05:22:39.418052Z"},"papermill":{"duration":0.015604,"end_time":"2024-04-30T13:49:34.790623","exception":false,"start_time":"2024-04-30T13:49:34.775019","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3_5\"></a>\n# <b> <span style='color:dimgray'>Step 3.5 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Remove Numeric Digit</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"**Remove Numeric Digits:** Numeric digits are often irrelevant for text classification tasks and can be removed.\n\n**Example:**  \n**Input:** \"I have 10 apples.\"  \n**Output:** \"I have apples.\"\n","metadata":{}},{"cell_type":"code","source":"def remove_numbers(text):\n    \"\"\"Exclude numerical digits from the text.\"\"\"\n    return re.sub(r'\\d+', '', text)\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:39.429446Z","iopub.status.busy":"2024-05-03T05:22:39.4291Z","iopub.status.idle":"2024-05-03T05:22:39.439493Z","shell.execute_reply":"2024-05-03T05:22:39.438407Z","shell.execute_reply.started":"2024-05-03T05:22:39.429417Z"},"papermill":{"duration":0.016339,"end_time":"2024-04-30T13:49:34.837923","exception":false,"start_time":"2024-04-30T13:49:34.821584","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3_6\"></a>\n# <b> <span style='color:dimgray'>Step 3.6 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Remove Punctuation</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"**Remove Punctuation:** Punctuation marks such as periods, commas, or exclamation marks can be removed as they usually don't carry important semantic information.\n\n**Example:**  \n**Input:** \"Hello, how are you?\"  \n**Output:** \"Hello how are you\"\n","metadata":{}},{"cell_type":"code","source":"def remove_punctuation(text):\n    \"\"\"Remove punctuation marks from the text.\"\"\"\n    return text.translate(str.maketrans('', '', string.punctuation))\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:39.441461Z","iopub.status.busy":"2024-05-03T05:22:39.440995Z","iopub.status.idle":"2024-05-03T05:22:39.452193Z","shell.execute_reply":"2024-05-03T05:22:39.450849Z","shell.execute_reply.started":"2024-05-03T05:22:39.441425Z"},"papermill":{"duration":0.016166,"end_time":"2024-04-30T13:49:34.861839","exception":false,"start_time":"2024-04-30T13:49:34.845673","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3_7\"></a>\n# <b> <span style='color:dimgray'>Step 3.7 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Split Text In Token</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"**Split Text into Tokens:** This step involves splitting the text into individual words or tokens. Tokens serve as the basic units for further analysis.\n\n**Example:**  \n**Input:** \"I love natural language processing.\"  \n**Output:** [\"I\", \"love\", \"natural\", \"language\", \"processing\"]\n","metadata":{}},{"cell_type":"code","source":"def tokenize_text(text):\n    \"\"\"Split the text into individual words or tokens.\"\"\"\n    return word_tokenize(text)\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:39.454101Z","iopub.status.busy":"2024-05-03T05:22:39.453673Z","iopub.status.idle":"2024-05-03T05:22:39.468096Z","shell.execute_reply":"2024-05-03T05:22:39.466828Z","shell.execute_reply.started":"2024-05-03T05:22:39.45406Z"},"papermill":{"duration":0.015779,"end_time":"2024-04-30T13:49:34.885165","exception":false,"start_time":"2024-04-30T13:49:34.869386","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3_8\"></a>\n# <b> <span style='color:dimgray'>Step 3.8 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Eliminate Stopwords</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"**Eliminate Stop Words:** Stop words are commonly used words like \"the,\" \"is,\" or \"and\" that don't carry much meaning and can be removed to reduce noise in the data.\n\n**Example:**  \n**Input:** \"The quick brown fox jumps over the lazy dog.\"  \n**Output:** [\"quick\", \"brown\", \"fox\", \"jumps\", \"lazy\", \"dog\"]\n","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(tokens):\n    \"\"\"Eliminate common stopwords from the tokenized text.\"\"\"\n    stop_words = set(stopwords.words('english'))\n    return [word for word in tokens if word not in stop_words]\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:39.469951Z","iopub.status.busy":"2024-05-03T05:22:39.469572Z","iopub.status.idle":"2024-05-03T05:22:39.480267Z","shell.execute_reply":"2024-05-03T05:22:39.479236Z","shell.execute_reply.started":"2024-05-03T05:22:39.469921Z"},"papermill":{"duration":0.016002,"end_time":"2024-04-30T13:49:34.9088","exception":false,"start_time":"2024-04-30T13:49:34.892798","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3_9\"></a>\n# <b> <span style='color:dimgray'>Step 3.9 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Remove Emojis</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"**Remove Emoji:** Emojis are graphical representations that may not contribute much to the classification task and can be removed.\n\n**Example:**  \n**Input:** \"I'm feeling üòä today.\"  \n**Output:** \"I'm feeling today.\"\n","metadata":{}},{"cell_type":"code","source":"import re\n\ndef remove_emojis(text):\n    \"\"\"Remove emojis from the text.\"\"\"\n    if isinstance(text, str):\n        emoji_pattern = re.compile(\"[\"\n                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                                   u\"\\U00002500-\\U00002BEF\"  # chinese char\n                                   u\"\\U00002702-\\U000027B0\"\n                                   u\"\\U00002702-\\U000027B0\"\n                                   u\"\\U000024C2-\\U0001F251\"\n                                   u\"\\U0001f926-\\U0001f937\"\n                                   u\"\\U00010000-\\U0010ffff\"\n                                   u\"\\u2640-\\u2642\"\n                                   u\"\\u2600-\\u2B55\"\n                                   u\"\\u200d\"\n                                   u\"\\u23cf\"\n                                   u\"\\u23e9\"\n                                   u\"\\u231a\"\n                                   u\"\\ufe0f\"  # dingbats\n                                   u\"\\u3030\"\n                                   \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', text)\n    else:\n        return text\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:39.482047Z","iopub.status.busy":"2024-05-03T05:22:39.48165Z","iopub.status.idle":"2024-05-03T05:22:39.493392Z","shell.execute_reply":"2024-05-03T05:22:39.49212Z","shell.execute_reply.started":"2024-05-03T05:22:39.482017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3_10\"></a>\n# <b> <span style='color:dimgray'>Step 3.10 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Vectorize Data</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"**Vectorized Data:** Text data needs to be converted into numerical vectors for machine learning algorithms to process. Various techniques like Bag-of-Words, TF-IDF, or word embeddings can be used for vectorizing the text.\n\n**Example:**  \n**Input:** [\"I\", \"love\", \"OpenAI\"]  \n**Output:** [0, 1, 1, 0, 0, 0, 0, 0, 1, 0, ...] (vector representation)\n","metadata":{}},{"cell_type":"code","source":"def vectorize_data(text_data):\n    # Join the tokenized text into strings\n    text_data_strings = [\" \".join(tokens) for tokens in text_data]\n    # Initialize TfidfVectorizer\n    tfidf_vectorizer = TfidfVectorizer()\n    # Fit and transform the text data to generate TF-IDF vectors\n    tfidf_vectors = tfidf_vectorizer.fit_transform(text_data_strings)\n    return tfidf_vectors, tfidf_vectorizer\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:39.495045Z","iopub.status.busy":"2024-05-03T05:22:39.4947Z","iopub.status.idle":"2024-05-03T05:22:39.506355Z","shell.execute_reply":"2024-05-03T05:22:39.505271Z","shell.execute_reply.started":"2024-05-03T05:22:39.495016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef preprocess_text(df):\n    df = filter_non_string(df, 'Tweet')\n    df['Tweet'] = df['Tweet'].apply(normalize_text)\n    df['Tweet'] = df['Tweet'].apply(remove_html_tags)\n    df['Tweet'] = df['Tweet'].apply(remove_urls)\n    df['Tweet'] = df['Tweet'].apply(remove_numbers)\n    df['Tweet'] = df['Tweet'].apply(remove_punctuation)\n    df['Tweet'] = df['Tweet'].apply(tokenize_text)\n    df['Tweet'] = df['Tweet'].apply(remove_stopwords)\n    df['Tweet'] = df['Tweet'].apply(remove_emojis)\n    return df\n\n# Usage:\ndf_processed = preprocess_text(df)","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:22:39.512394Z","iopub.status.busy":"2024-05-03T05:22:39.511991Z","iopub.status.idle":"2024-05-03T05:23:11.335909Z","shell.execute_reply":"2024-05-03T05:23:11.334807Z","shell.execute_reply.started":"2024-05-03T05:22:39.512328Z"},"papermill":{"duration":21.903389,"end_time":"2024-04-30T13:49:56.843417","exception":false,"start_time":"2024-04-30T13:49:34.940028","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <b> <span style='color:dimgray'>Step 4 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Build Model</span></b>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4_1\"></a>\n# <b> <span style='color:dimgray'>Step 4.1 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Train Split Data</span></b>\n","metadata":{}},{"cell_type":"code","source":"\n# df_processed contains preprocessed text data\n# Convert list of words into a single string for each entry in 'Tweet' column\ndf_processed['Tweet'] = df_processed['Tweet'].apply(lambda x: ' '.join(x))\n\n# Split data into features (X) and target (y)\nX = df_processed['Tweet']  # Features\ny = df_processed['Sentiment']  # Target\n\n# Split data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print the shapes of the training and testing sets\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of y_test:\", y_test.shape)\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:23:11.337712Z","iopub.status.busy":"2024-05-03T05:23:11.337383Z","iopub.status.idle":"2024-05-03T05:23:11.465661Z","shell.execute_reply":"2024-05-03T05:23:11.464726Z","shell.execute_reply.started":"2024-05-03T05:23:11.337682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4_2\"></a>\n# <b> <span style='color:dimgray'>Step 4.2 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>RandomForestClassifier</span></b>\n","metadata":{}},{"cell_type":"code","source":"\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert text data to TF-IDF features\nvectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\n\n# Train Random Forest classifier\nrf_classifier = RandomForestClassifier(random_state=42)\nrf_classifier.fit(X_train_tfidf, y_train)\n\n# Predict on the testing data\ny_pred = rf_classifier.predict(X_test_tfidf)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Test Accuracy:\", accuracy)\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:23:11.467944Z","iopub.status.busy":"2024-05-03T05:23:11.467435Z","iopub.status.idle":"2024-05-03T05:28:47.608275Z","shell.execute_reply":"2024-05-03T05:28:47.606791Z","shell.execute_reply.started":"2024-05-03T05:23:11.467904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4_3\"></a>\n# <b> <span style='color:dimgray'>Step 4.3 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Confusion Matrix</span></b>\n","metadata":{}},{"cell_type":"code","source":"# Create a confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n# Create a classification report\nclass_report = classification_report(y_test, y_pred)\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(class_report)\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:28:47.611245Z","iopub.status.busy":"2024-05-03T05:28:47.610697Z","iopub.status.idle":"2024-05-03T05:28:48.777786Z","shell.execute_reply":"2024-05-03T05:28:48.775475Z","shell.execute_reply.started":"2024-05-03T05:28:47.611194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4_4\"></a>\n# <b> <span style='color:dimgray'>Step 4.4 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Actual VS Predicred</span></b>\n","metadata":{}},{"cell_type":"code","source":"\n# Print some actual vs predicted labels along with tweet text\nprint(\"Actual vs Predicted Labels with Tweet Text:\")\nfor tweet, actual_label, predicted_label in zip(X_test[:10], y_test[:10], y_pred[:10]):\n    print(\"Tweet:\", tweet)\n    print(\"Actual Label:\", actual_label)\n    print(\"Predicted Label:\", predicted_label)\n    print(\"-----------------------\")\n","metadata":{"execution":{"iopub.execute_input":"2024-05-03T05:28:48.781374Z","iopub.status.busy":"2024-05-03T05:28:48.780647Z","iopub.status.idle":"2024-05-03T05:28:48.796248Z","shell.execute_reply":"2024-05-03T05:28:48.793887Z","shell.execute_reply.started":"2024-05-03T05:28:48.781298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4_5\"></a>\n# <b> <span style='color:dimgray'>Step 4.5 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Save Model</span></b>\n","metadata":{}},{"cell_type":"code","source":"import joblib\n\n# Save the trained model\njoblib.dump(rf_classifier, 'rf_model.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4_6\"></a>\n# <b> <span style='color:dimgray'>Step 4.6 </span> <span style='color:crimson'>|</span> <span style='color:#53599A'>Predict Random Tweet By Model</span></b>\n","metadata":{}},{"cell_type":"code","source":"\n# Load the saved model\nTweet_Sentiment_Ai = joblib.load('rf_model.pkl')\n\n# Define a mapping between sentiment labels and emojis\nsentiment_emojis = {\n    'Positive': 'üòÑ',\n    'Negative': 'üòû',\n    'Neutral': 'üòê',\n    'Irrelevant': 'ü§∑‚Äç‚ôÇÔ∏è'\n}\n\n# Example tweets\nexample_tweets = [\n    \"I just finished playing Borderlands and it was absolutely amazing! Can't wait for the next one!\",\n    \"I'm really disappointed with the latest Borderlands update. It ruined the game for me.\",\n    \"Haven't played Borderlands in a while. Need to catch up on the latest updates.\",\n    \"Just saw a funny cat video while searching for Borderlands gameplay. Cats always cheer me up!\"\n]\n\n\n# Transform example tweets into TF-IDF features\nexample_tweets_tfidf = vectorizer.transform(example_tweets)\n\n# Predict sentiment for example tweets\npredictions = Tweet_Sentiment_Ai.predict(example_tweets_tfidf)\n\n# Print the predicted sentiment and corresponding emoji for each example tweet\nfor tweet, prediction in zip(example_tweets, predictions):\n    sentiment = prediction\n    emoji = sentiment_emojis[sentiment]  # Get the emoji directly from the mapping without a default value\n    print(\"Tweet:\", tweet)\n    print(\"Sentiment:\", sentiment)\n    print(\"Emoji:\", emoji)\n    print()","metadata":{"execution":{"iopub.execute_input":"2024-05-03T06:37:15.94829Z","iopub.status.busy":"2024-05-03T06:37:15.947889Z","iopub.status.idle":"2024-05-03T06:37:17.375484Z","shell.execute_reply":"2024-05-03T06:37:17.374371Z","shell.execute_reply.started":"2024-05-03T06:37:15.948258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://i.gifer.com/7ARb.gif\" style=\"width:100%\">","metadata":{}},{"cell_type":"markdown","source":"# **THANK YOU !**","metadata":{}}]}